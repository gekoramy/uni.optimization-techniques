{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Optimization techniques Lab. 6: Bayesian Optimization\n",
    "## Introduction\n",
    "**Goal.** The goal of this lab is to study the behavior of Bayesian optimization on a regression problem and a classifier one. \n",
    "Bayesian optimization is a probabilistic approach that uses the Bayes' Theorem $P(A|B) = \\frac{P(B|A)*P(A)}{P(B)}$. Briefly, we use the prior information, $P(A)$,(random samples) to optimize a surrogate function, $P(B|A)$.\n",
    "\n",
    "**Getting started.** The following cells contain the implementation of the methods that we will use throughout this lab, together with utilities. \n"
   ],
   "metadata": {
    "id": "eK4fQ2q-Xcx1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from warnings import catch_warnings, simplefilter\n",
    "from matplotlib import pyplot\n",
    "from numpy import arange, ndarray, sin, argmax, asarray, mean, vstack\n",
    "from numpy.random import normal, random\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Integer\n",
    "from skopt.utils import use_named_args"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hyHpT2yLjqsu"
   },
   "outputs": [],
   "source": [
    "def surrogate(model: GaussianProcessRegressor, X: ndarray[float]) -> Tuple[ndarray[float], ndarray[float]]:\n",
    "    \"\"\"\n",
    "    surrogate or approximation for the objective function\n",
    "\n",
    "    :param model:\n",
    "    :param X:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # catch any warning generated when making a prediction\n",
    "    with catch_warnings():\n",
    "        # ignore generated warnings\n",
    "        simplefilter(\"ignore\")\n",
    "        return model.predict(X, return_std=True)\n",
    "\n",
    "\n",
    "def acquisition(X: ndarray[float], Xsamples: ndarray[float], model: GaussianProcessRegressor) -> float:\n",
    "    # calculate the best surrogate score found so far\n",
    "    yhat, _ = surrogate(model, X)\n",
    "    print(X)\n",
    "    best = max(yhat)\n",
    "    # calculate mean and stdev via surrogate function\n",
    "    mu, std = surrogate(model, Xsamples)\n",
    "    # mu = mu[:, 0]\n",
    "    # calculate the probability of improvement\n",
    "    probs = acquisition_function()\n",
    "    return probs\n",
    "\n",
    "\n",
    "def opt_acquisition(X: ndarray[float], y: ndarray[float], model: GaussianProcessRegressor) -> float:\n",
    "    \"\"\"\n",
    "    optimize the acquisition function\n",
    "\n",
    "    :param X:\n",
    "    :param y:\n",
    "    :param model:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # random search, generate random samples\n",
    "    Xsamples: ndarray[float] = random(100)\n",
    "    Xsamples = Xsamples.reshape(len(Xsamples), 1)\n",
    "    # calculate the acquisition function for each sample\n",
    "    scores = acquisition(X, Xsamples, model)\n",
    "    # locate the index of the largest scores\n",
    "    ix = argmax(scores)\n",
    "    return Xsamples[ix, 0]\n",
    "\n",
    "\n",
    "def plot(X: ndarray[float], y: ndarray[float], model: GaussianProcessRegressor) -> None:\n",
    "    \"\"\"\n",
    "    plot real observations vs surrogate function\n",
    "\n",
    "    :param X:\n",
    "    :param y:\n",
    "    :param model:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # scatter plot of inputs and real objective function\n",
    "    pyplot.scatter(X, y)\n",
    "    # line plot of surrogate function across domain\n",
    "    Xsamples = asarray(arange(0, 1, 0.001))\n",
    "    Xsamples = Xsamples.reshape(len(Xsamples), 1)\n",
    "    ysamples, _ = surrogate(model, Xsamples)\n",
    "    pyplot.plot(Xsamples, ysamples)\n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    "    pyplot.close()\n",
    "\n",
    "\n",
    "def bayesian_optimization(generation: int) -> Tuple[ndarray[float], ndarray[float], GaussianProcessRegressor]:\n",
    "    # sample the domain sparsely with noise\n",
    "    xs: ndarray[float]\n",
    "    ys: ndarray[float]\n",
    "    xs, ys = initial_point()\n",
    "    # reshape into rows and cols\n",
    "    xs = xs.reshape(len(xs), 1)\n",
    "    ys = ys.reshape(len(ys), 1)\n",
    "    # define the model\n",
    "    model: GaussianProcessRegressor = GaussianProcessRegressor()  # you can set the kernel and the optimizer\n",
    "    # fit the model\n",
    "    model.fit(xs, ys)\n",
    "    # perform the optimization process\n",
    "    for i in range(generation):\n",
    "        # select the next point to sample\n",
    "        x = opt_acquisition(xs, ys, model)\n",
    "        # sample the point\n",
    "        actual: float = objective(x)\n",
    "        # summarize the finding\n",
    "        est, _ = surrogate(model, [[x]])\n",
    "        # add the data to the dataset\n",
    "        xs = vstack((xs, [[x]]))\n",
    "        ys = vstack((ys, [[actual]]))\n",
    "        # update the model\n",
    "        model.fit(xs, ys)\n",
    "    return xs, ys, model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Implementative part.\n",
    "Your first step, will be to implement the following functions:\n",
    "\n",
    "\n",
    "1.   objective() is the function to optimize. \n",
    "2.   initial_point() returns the initial set of points (a priori knowledge)\n",
    "3.   acquisition_function() implements the acquisition function\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "_HsCKJjDVMYn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# objective function\n",
    "def objective(x: float, noise: float = 0.1) -> float:\n",
    "    return sin(x * 10) + normal(loc=0, scale=noise)\n",
    "\n",
    "\n",
    "# remember to return the value in the right order and type\n",
    "def initial_point(size: int = 2) -> Tuple[ndarray[float], ndarray[float]]:\n",
    "    xs: ndarray[float] = random(size)\n",
    "    ys: ndarray[float] = asarray([objective(x) for x in xs])\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "# you have to add the parameters that you need\n",
    "def acquisition_function() -> float:\n",
    "    return 0"
   ],
   "metadata": {
    "id": "0QIefWBRTLCH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Regression\n",
    "---\n",
    "## Questions:\n",
    "- How does the prior knowledge change the optimization?\n",
    "- How does the kernel change the optimization? (see here the [kernels](https://scikit-learn.org/stable/modules/gaussian_process.html#kernels-for-gaussian-processes))\n",
    "- How does the acquisition function affect the optimization?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def regression() -> None :\n",
    "    X: ndarray[float]\n",
    "    y: ndarray[float]\n",
    "    model: GaussianProcessRegressor\n",
    "    X, y, model = bayesian_optimization(10)\n",
    "    plot(X, y, model)\n",
    "    # best result\n",
    "    ix: ndarray[int] = argmax(y)\n",
    "    print('Best Result: x=%.3f, y=%.3f' % (X[ix], y[ix]))\n",
    "\n",
    "\n",
    "regression()"
   ],
   "metadata": {
    "id": "KNEsNnVYuzE0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "outputId": "b4f67830-878d-463c-d3ef-1a4acc266b7a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classifier\n",
    "---\n",
    "## Questions:\n",
    "- Try different ranges of hyperparameters. How do the results change?\n",
    "- Does the model influence the choice of the hyperparameters?"
   ],
   "metadata": {
    "id": "dzw4lTn9A5MK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def classifier() -> None:\n",
    "    # generate 2d classification dataset\n",
    "    X, y = make_blobs(n_samples=500, centers=3, n_features=2)\n",
    "    # define the model\n",
    "\n",
    "    model = KNeighborsClassifier()\n",
    "    # define the space of hyperparameters to search\n",
    "    search_space = [Integer(1, 5, name='n_neighbors'), Integer(1, 2, name='p')]\n",
    "\n",
    "    # define the function used to evaluate a given configuration\n",
    "    @use_named_args(search_space)\n",
    "    def evaluate_model(**params):\n",
    "        # something\n",
    "        model.set_params(**params)\n",
    "        # calculate 5-fold cross validation\n",
    "        with catch_warnings():\n",
    "            # ignore generated warnings\n",
    "            simplefilter(\"ignore\")\n",
    "            result = cross_val_score(model, X, y, cv=5, n_jobs=-1, scoring='accuracy')\n",
    "            # calculate the mean of the scores\n",
    "            estimate = mean(result)\n",
    "            return 1.0 - estimate\n",
    "\n",
    "    # perform optimization\n",
    "    result = gp_minimize(evaluate_model, search_space)\n",
    "    # summarizing finding:\n",
    "    print('Best Accuracy: %.3f' % (1.0 - result.fun))\n",
    "    print('Best Parameters: n_neighbors=%d, p=%d' % (result.x[0], result.x[1]))\n",
    "\n",
    "\n",
    "classifier()"
   ],
   "metadata": {
    "id": "jQBZkkzBvR3Z"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BONUS\n",
    "\n",
    "You see in the classifier the effect of hyperparameter tuning. \n",
    "You can now change the acquisition functions in the regression problem, adding a slack variable as a hyperparameter. How does this variable affect the optimization problem?"
   ],
   "metadata": {
    "id": "EvVHVIoCqLWX"
   }
  }
 ]
}
