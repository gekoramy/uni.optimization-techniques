{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Optimization techniques Lab. 6: Bayesian Optmization\n",
        "## Introduction\n",
        "**Goal.** The goal of this lab is to study the behavior of Bayesian optimization on a regression problem and a classifier one. \n",
        "Bayesian optimization is a probabilistic approach that uses the Bayes' Theorem $P(A|B) = \\frac{P(B|A)*P(A)}{P(B)}. Briefly, we use the prior information, P(A),(random samples) to optimize a surrogate function, P(B|A).\n",
        "\n",
        "**Getting started.** The following cells contain the implementation of the methods that we will use throughout this lab, together with utilities. \n"
      ],
      "metadata": {
        "id": "eK4fQ2q-Xcx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimize"
      ],
      "metadata": {
        "id": "R5uQkLcFS1GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of bayesian optimization for a 1d function from scratch\n",
        "from math import sin\n",
        "from math import pi\n",
        "from numpy import arange\n",
        "from numpy import vstack\n",
        "from numpy import argmax\n",
        "from numpy import asarray\n",
        "from numpy.random import normal\n",
        "from numpy.random import random\n",
        "from scipy.stats import norm\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from warnings import catch_warnings\n",
        "from warnings import simplefilter\n",
        "from matplotlib import pyplot\n",
        "\n",
        "from numpy import mean\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from skopt.space import Integer\n",
        "from skopt.utils import use_named_args\n",
        "from warnings import catch_warnings\n",
        "from skopt import gp_minimize\n",
        "from warnings import simplefilter"
      ],
      "metadata": {
        "id": "kttrGnJOBvhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyHpT2yLjqsu"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# surrogate or approximation for the objective function\n",
        "def surrogate(model, X):\n",
        "\t# catch any warning generated when making a prediction\n",
        "\twith catch_warnings():\n",
        "\t\t# ignore generated warnings\n",
        "\t\tsimplefilter(\"ignore\")\n",
        "\t\treturn model.predict(X, return_std=True)\n",
        "\n",
        "def acquisition(X, Xsamples, model):\n",
        "\t# calculate the best surrogate score found so far\n",
        "\tyhat, _ = surrogate(model, X)\n",
        "\tprint(X)\n",
        "\tbest = max(yhat)\n",
        "\t# calculate mean and stdev via surrogate function\n",
        "\tmu, std = surrogate(model, Xsamples)\n",
        "\tmu = mu[:, 0]\n",
        "\t# calculate the probability of improvement\n",
        "\tprobs = acquisition_function()\n",
        "\treturn probs\n",
        "\n",
        "# optimize the acquisition function\n",
        "def opt_acquisition(X, y, model):\n",
        "\t# random search, generate random samples\n",
        "\tXsamples = random(100)\n",
        "\tXsamples = Xsamples.reshape(len(Xsamples), 1)\n",
        "\t# calculate the acquisition function for each sample\n",
        "\tscores = acquisition(X, Xsamples, model)\n",
        "\t# locate the index of the largest scores\n",
        "\tix = argmax(scores)\n",
        "\treturn Xsamples[ix, 0]\n",
        "\n",
        "# plot real observations vs surrogate function\n",
        "def plot(X, y, model):\n",
        "\t# scatter plot of inputs and real objective function\n",
        "\tpyplot.scatter(X, y)\n",
        "\t# line plot of surrogate function across domain\n",
        "\tXsamples = asarray(arange(0, 1, 0.001))\n",
        "\tXsamples = Xsamples.reshape(len(Xsamples), 1)\n",
        "\tysamples, _ = surrogate(model, Xsamples)\n",
        "\tpyplot.plot(Xsamples, ysamples)\n",
        "\t# show the plot\n",
        "\tpyplot.show()\n",
        "\n",
        "\n",
        "def bayesianOptmization(generation):\n",
        "    # sample the domain sparsely with noise\n",
        "    X , y  = initial_point(size = 2)\n",
        "    # reshape into rows and cols\n",
        "    X = X.reshape(len(X), 1)\n",
        "    y = y.reshape(len(y), 1)\n",
        "    # define the model\n",
        "    model = GaussianProcessRegressor() #you can set the kernel and the optmizer \n",
        "    # fit the model\n",
        "    model.fit(X, y)\n",
        "    # perform the optimization process\n",
        "    for i in range(generation):\n",
        "        # select the next point to sample\n",
        "        x = opt_acquisition(X, y, model)\n",
        "        # sample the point\n",
        "        actual = objective(x)\n",
        "        # summarize the finding\n",
        "        est, _ = surrogate(model, [[x]])\n",
        "        # add the data to the dataset\n",
        "        X = vstack((X, [[x]]))\n",
        "        y = vstack((y, [[actual]]))\n",
        "        # update the model\n",
        "        model.fit(X, y)\n",
        "    return X, y, model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implementative part.\n",
        "Your first step, will be to implement the following functions:\n",
        "\n",
        "\n",
        "1.   objective() is the function to optimize. \n",
        "2.   initial_point() returns the initial set of points (a priori knowledge)\n",
        "3.   acquisition_function() implements the acquisition function\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_HsCKJjDVMYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# objective function\n",
        "def objective(x, noise=0.1):\n",
        "\treturn x + normal(loc=0, scale=noise)\n",
        "\n",
        "#remember to return the value in the right order and type.\n",
        "def initial_point(size = 2):\n",
        "    X = random(size)\n",
        "    Y = asarray([objective(x) for x in X])\n",
        "    return X, Y\n",
        "\n",
        "#you have to add the parameters that you need.\n",
        "def acquisition_function():\n",
        "    return 0"
      ],
      "metadata": {
        "id": "0QIefWBRTLCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression\n",
        "---\n",
        "## Questions:\n",
        "- How does the prior knowledge change the optimization?\n",
        "- How does the kernel change the optimization? (see here the [kernels](https://scikit-learn.org/stable/modules/gaussian_process.html#kernels-for-gaussian-processes))\n",
        "- ow does the acquisition function affect the optimization?"
      ],
      "metadata": {
        "id": "B8swyFr8AYZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y, model = bayesianOptmization(1)\n",
        "plot(X, y, model)\n",
        "# best result\n",
        "ix = argmax(y)\n",
        "print('Best Result: x=%.3f, y=%.3f' % (X[ix], y[ix]))"
      ],
      "metadata": {
        "id": "KNEsNnVYuzE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classifier\n",
        "---\n",
        "## Questions:\n",
        "- Try different ranges of hyperparameters. How do the results change?\n",
        "- Does the model influence the choice of the hyperparameters?\n",
        "\n"
      ],
      "metadata": {
        "id": "dzw4lTn9A5MK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "        \n",
        "# generate 2d classification dataset\n",
        "X, y = make_blobs(n_samples=500, centers=3, n_features=2)\n",
        "# define the model\n",
        "\n",
        "model = KNeighborsClassifier()\n",
        "# define the space of hyperparameters to search\n",
        "search_space = [Integer(1, 5, name='n_neighbors'), Integer(1, 2, name='p')]\n",
        "# define the function used to evaluate a given configuration\n",
        "@use_named_args(search_space)\n",
        "def evaluate_model(**params):\n",
        "    # something\n",
        "    model.set_params(**params)\n",
        "    # calculate 5-fold cross validation\n",
        "    with catch_warnings():\n",
        "        # ignore generated warnings\n",
        "        simplefilter(\"ignore\")\n",
        "        result = cross_val_score(model, X, y, cv=5, n_jobs=-1, scoring='accuracy')\n",
        "        # calculate the mean of the scores\n",
        "        estimate = mean(result)\n",
        "        return 1.0 - estimate\n",
        "\n",
        "# perform optimization\n",
        "result = gp_minimize(evaluate_model, search_space)\n",
        "# summarizing finding:\n",
        "print('Best Accuracy: %.3f' % (1.0 - result.fun))\n",
        "print('Best Parameters: n_neighbors=%d, p=%d' % (result.x[0], result.x[1]))"
      ],
      "metadata": {
        "id": "jQBZkkzBvR3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BONUS\n",
        "\n",
        "You see in the classifier the effect of hyperparameter tuning. \n",
        "You can now change the acquisition functions in the regression problem, adding a slack variable as a hyperparameter. How does this variable affect the optimization problem?"
      ],
      "metadata": {
        "id": "EvVHVIoCqLWX"
      }
    }
  ]
}