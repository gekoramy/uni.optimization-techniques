{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Optimization techniques Lab. 6: Bayesian Optimization\n",
    "## Introduction\n",
    "**Goal.** The goal of this lab is to study the behavior of Bayesian optimization on a regression problem and a classifier one. \n",
    "Bayesian optimization is a probabilistic approach that uses the Bayes' Theorem $P(A|B) = \\frac{P(B|A)*P(A)}{P(B)}$. Briefly, we use the prior information, $P(A)$,(random samples) to optimize a surrogate function, $P(B|A)$.\n",
    "\n",
    "**Getting started.** The following cells contain the implementation of the methods that we will use throughout this lab, together with utilities. \n"
   ],
   "metadata": {
    "id": "eK4fQ2q-Xcx1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from typing import Tuple, Callable, List\n",
    "from warnings import catch_warnings, simplefilter\n",
    "from matplotlib import pyplot\n",
    "from numpy import arange, ndarray, sin, argmax, asarray, mean, vstack, pi\n",
    "from numpy.random import normal, random\n",
    "from scipy.stats import norm\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ExpSineSquared, Matern, RationalQuadratic, DotProduct\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Integer\n",
    "from skopt.utils import use_named_args"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# objective function\n",
    "# def objective(x: float, noise: float = 0.1) -> float:\n",
    "#     return x ** 2 * sin(3 * pi * x) + np.random.uniform(-noise, noise)\n",
    "\n",
    "def objective(x: float, noise: float = 0.05) -> float:\n",
    "    return sin(3 * 2 * pi * x) + np.random.uniform(-noise, noise)"
   ],
   "metadata": {
    "id": "0QIefWBRTLCH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "XS = arange(0, 1, 0.001).reshape(-1, 1)\n",
    "YS = objective(XS, 0)\n",
    "\n",
    "\n",
    "def surrogate(model: GaussianProcessRegressor, X: ndarray[float]) -> Tuple[ndarray[float], ndarray[float]]:\n",
    "    \"\"\"\n",
    "    surrogate or approximation for the objective function\n",
    "    \"\"\"\n",
    "\n",
    "    # catch any warning generated when making a prediction\n",
    "    with catch_warnings():\n",
    "        # ignore generated warnings\n",
    "        simplefilter(\"ignore\")\n",
    "        return model.predict(X, return_std=True)\n",
    "\n",
    "\n",
    "def opt_acquisition(\n",
    "        xs_known: ndarray[float],\n",
    "        ys_known: ndarray[float],\n",
    "        model: GaussianProcessRegressor,\n",
    "        acquisition: Callable[[ndarray[float], ndarray[float], GaussianProcessRegressor], ndarray[float]]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    optimize the acquisition function\n",
    "    \"\"\"\n",
    "\n",
    "    # random search, generate random samples\n",
    "    xs_unknown: ndarray[float] = random(50)\n",
    "    xs_unknown = xs_unknown.reshape(-1, 1)\n",
    "\n",
    "    # calculate the acquisition function for each sample\n",
    "    scores = acquisition(xs_known, xs_unknown, model)\n",
    "\n",
    "    # locate the index of the largest scores\n",
    "    ix = argmax(scores)\n",
    "    return xs_unknown[ix, 0]\n",
    "\n",
    "\n",
    "def bayesian_optimization(\n",
    "        generation: int,\n",
    "        model: GaussianProcessRegressor,\n",
    "        acquisition: Callable[[ndarray[float], ndarray[float], GaussianProcessRegressor], ndarray[float]],\n",
    "        initial_points: List[float],\n",
    "        file: str\n",
    ") -> Tuple[ndarray[float], ndarray[float], GaussianProcessRegressor]:\n",
    "    # reshape into rows and cols\n",
    "    xs_known = asarray(initial_points).reshape(-1, 1)\n",
    "    ys_known = asarray([objective(x) for x in xs_known]).reshape(-1, 1)\n",
    "\n",
    "    # fit the model\n",
    "    model.fit(xs_known, ys_known)\n",
    "\n",
    "    # perform the optimization process\n",
    "    for i in range(generation):\n",
    "        # select the next point\n",
    "        # and sample it\n",
    "        x_next = opt_acquisition(xs_known, ys_known, model, acquisition)\n",
    "        y_next = objective(x_next)\n",
    "\n",
    "        # region plot\n",
    "        fig, (ax1, ax2) = pyplot.subplots(2, sharex=True, height_ratios=[3, 1], gridspec_kw={'hspace': 0})\n",
    "        plot_approximation(ax1, model, xs_known, ys_known, x_next)\n",
    "        plot_acquisition(ax2, model, xs_known, acquisition, x_next)\n",
    "        if i == 0:\n",
    "            lines_labels = [ax.get_legend_handles_labels() for ax in fig.axes]\n",
    "            lines, labels = [sum(lol, []) for lol in zip(*lines_labels)]\n",
    "            fig.legend(lines, labels, loc='upper left')\n",
    "        pyplot.savefig(f'{file} {i}.svg')\n",
    "        # endregion\n",
    "\n",
    "        # add the data to the dataset\n",
    "        xs_known = vstack((xs_known, [[x_next]]))\n",
    "        ys_known = vstack((ys_known, [[y_next]]))\n",
    "\n",
    "        # update the model\n",
    "        model.fit(xs_known, ys_known)\n",
    "\n",
    "    pyplot.close()\n",
    "    return xs_known, ys_known, model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_approximation(\n",
    "        ax,\n",
    "        model,\n",
    "        xs_known,\n",
    "        ys_known,\n",
    "        x_next,\n",
    "):\n",
    "    mu, std = model.predict(XS, return_std=True)\n",
    "    ax.fill_between(\n",
    "        XS.ravel(),\n",
    "        mu.ravel() + 1.96 * std,\n",
    "        mu.ravel() - 1.96 * std,\n",
    "        alpha=0.1\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        XS.ravel(),\n",
    "        YS.ravel() + 0.05,\n",
    "        YS.ravel() - 0.05,\n",
    "        alpha=0.1\n",
    "    )\n",
    "    ax.plot(XS, YS, 'y--', lw=1, label='objective')\n",
    "    ax.plot(XS, mu, 'b-', lw=1, label='surrogate function')\n",
    "    ax.plot(xs_known, ys_known, 'kx', mew=3, label='noisy samples')\n",
    "    ax.axvline(x=x_next, ls='--', c='k', lw=1)\n",
    "\n",
    "\n",
    "def plot_acquisition(\n",
    "        ax,\n",
    "        model,\n",
    "        xs_known,\n",
    "        acquisition: Callable[[ndarray[float], ndarray[float], GaussianProcessRegressor], ndarray[float]],\n",
    "        x_next,\n",
    "):\n",
    "    ax.plot(XS, acquisition(xs_known, XS, model), 'r-', lw=1, label='Acquisition function')\n",
    "    ax.axvline(x=x_next, ls='--', c='k', lw=1, label='Next sampling location')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Implementation part\n",
    "---\n",
    "Your first step, will be to implement the following functions:\n",
    "\n",
    "1.   objective() is the function to optimize.\n",
    "2.   initial_point() returns the initial set of points (a priori knowledge)\n",
    "3.   acquisition_function() implements the acquisition function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def probability_of_improvement(\n",
    "        xs_known: ndarray[float],\n",
    "        xs_unknown: ndarray[float],\n",
    "        model: GaussianProcessRegressor\n",
    ") -> ndarray[float]:\n",
    "    y_hat, _ = surrogate(model, xs_known)\n",
    "    best = max(y_hat)\n",
    "\n",
    "    mu, sd = surrogate(model, xs_unknown)\n",
    "    z = (best - mu) / sd\n",
    "\n",
    "    return norm.cdf(-z)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def expected_improvement(\n",
    "        xs_known: ndarray[float],\n",
    "        xs_unknown: ndarray[float],\n",
    "        model: GaussianProcessRegressor,\n",
    ") -> ndarray[float]:\n",
    "    y_hat, _ = surrogate(model, xs_known)\n",
    "    best = max(y_hat)\n",
    "\n",
    "    mu, sd = surrogate(model, xs_unknown)\n",
    "    z = (best - mu) / sd\n",
    "\n",
    "    return (mu - best) * norm.cdf(-z) + sd * norm.pdf(-z)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Regression\n",
    "---\n",
    "## Questions:\n",
    "- How does the prior knowledge change the optimization?\n",
    "- How does the kernel change the optimization? (see here the [kernels](https://scikit-learn.org/stable/modules/gaussian_process.html#kernels-for-gaussian-processes))\n",
    "- How does the acquisition function affect the optimization?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def regression() -> None:\n",
    "    xs: ndarray[float]\n",
    "    ys: ndarray[float]\n",
    "    model: GaussianProcessRegressor\n",
    "    xs, ys, model = bayesian_optimization(\n",
    "        generation=10,\n",
    "        model=GaussianProcessRegressor(ExpSineSquared(length_scale=1/3, periodicity=1/3)),\n",
    "        acquisition=expected_improvement,\n",
    "        initial_points=[1/3],\n",
    "        file='ei/ei exp-sine-squared one-third'\n",
    "    )\n",
    "\n",
    "    ix: ndarray[int] = argmax(ys)\n",
    "    print('Best Result: x=%.3f, y=%.3f' % (xs[ix], ys[ix]))\n",
    "\n",
    "\n",
    "regression()"
   ],
   "metadata": {
    "id": "KNEsNnVYuzE0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "outputId": "b4f67830-878d-463c-d3ef-1a4acc266b7a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Classifier\n",
    "---\n",
    "## Questions:\n",
    "- Try different ranges of hyperparameters. How do the results change?\n",
    "- Does the model influence the choice of the hyperparameters?"
   ],
   "metadata": {
    "id": "dzw4lTn9A5MK"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def classifier() -> None:\n",
    "    # generate 2d classification dataset\n",
    "    X, y = make_blobs(n_samples=500, centers=3, n_features=2)\n",
    "    # define the model\n",
    "\n",
    "    model = KNeighborsClassifier()\n",
    "    # define the space of hyperparameters to search\n",
    "    search_space = [Integer(1, 5, name='n_neighbors'), Integer(1, 2, name='p')]\n",
    "\n",
    "    # define the function used to evaluate a given configuration\n",
    "    @use_named_args(search_space)\n",
    "    def evaluate_model(**params):\n",
    "        # something\n",
    "        model.set_params(**params)\n",
    "        # calculate 5-fold cross validation\n",
    "        with catch_warnings():\n",
    "            # ignore generated warnings\n",
    "            simplefilter(\"ignore\")\n",
    "            result = cross_val_score(model, X, y, cv=5, n_jobs=-1, scoring='accuracy')\n",
    "            # calculate the mean of the scores\n",
    "            estimate = mean(result)\n",
    "            return 1.0 - estimate\n",
    "\n",
    "    # perform optimization\n",
    "    result = gp_minimize(evaluate_model, search_space)\n",
    "    # summarizing finding:\n",
    "    print('Best Accuracy: %.3f' % (1.0 - result.fun))\n",
    "    print('Best Parameters: n_neighbors=%d, p=%d' % (result.x[0], result.x[1]))\n",
    "\n",
    "\n",
    "classifier()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BONUS\n",
    "\n",
    "You see in the classifier the effect of hyperparameter tuning. \n",
    "You can now change the acquisition functions in the regression problem, adding a slack variable as a hyperparameter. How does this variable affect the optimization problem?"
   ],
   "metadata": {
    "id": "EvVHVIoCqLWX"
   }
  }
 ]
}
