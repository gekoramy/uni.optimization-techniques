{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Lab 9: Design of Experiment\n",
    "In this lab, we will observe the effect of the DoE in the Bayesian optimization and Bio-inspired approaches.\n",
    "\n",
    "In the following (hidden) block, the utilities used for running the experiments are implemented.\n",
    "\n",
    "The list of available benchmark functions can be found at this [link](https://gitlab.com/luca.baronti/python_benchmark_functions)\n",
    "\n",
    "**NOTE**: When studying the effect of the parameters is *extremely* important to vary just one parameter at a time. Therefore, you are suggested to study one parameter by fixing all the others, and then moving to the next.\n",
    "\n",
    "Moreover, when comparing different algorithms, is *very* important to run each of them several times (e.g., 30) by using different initial random seeds.\n",
    "\n",
    "\n",
    "You will use the sampling technique seen in the lecture. In particular, you will have to implement:\n",
    "\n",
    "\n",
    "*   Random sampling\n",
    "*   The Halton sequence\n",
    "*   The full factorial sampling\n",
    "*   The Latin Hypercube sampling\n",
    "\n"
   ],
   "metadata": {
    "id": "1LFQ4RnHpEMu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install cma\n",
    "!pip install inspyred\n",
    "!pip install benchmark_functions\n",
    "!pip install scikit-optimize"
   ],
   "metadata": {
    "id": "HbFs_V7eGmL3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Code for the Bayesian Optimization, you have to reuse the code from Lab. 6 for the acquisition and the objective functions."
   ],
   "metadata": {
    "id": "qt7sD4i35yVu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#@title\n",
    "\n",
    "# example of bayesian optimization for a 1d function from scratch\n",
    "from math import sin\n",
    "from math import pi\n",
    "from numpy import arange\n",
    "from numpy import vstack\n",
    "from numpy import argmax\n",
    "from numpy import asarray\n",
    "from numpy.random import normal\n",
    "from numpy.random import random\n",
    "from scipy.stats import norm\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from warnings import catch_warnings\n",
    "from warnings import simplefilter\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from numpy import mean\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from skopt.space import Integer\n",
    "from skopt.utils import use_named_args\n",
    "from warnings import catch_warnings\n",
    "from skopt import gp_minimize\n",
    "from warnings import simplefilter\n",
    "\n",
    "\n",
    "# surrogate or approximation for the objective function\n",
    "def surrogate(model, X):\n",
    "    # catch any warning generated when making a prediction\n",
    "    with catch_warnings():\n",
    "        # ignore generated warnings\n",
    "        simplefilter(\"ignore\")\n",
    "        return model.predict(X, return_std=True)\n",
    "\n",
    "\n",
    "def acquisition(X, Xsamples, model):\n",
    "    # # # calculate the best surrogate score found so far\n",
    "    # yhat, _ = surrogate(model, X)\n",
    "    # print(X)\n",
    "    # best = max(yhat)\n",
    "    # # calculate mean and stdev via surrogate function\n",
    "    # mu, std = surrogate(model, Xsamples)\n",
    "    # mu = mu[:, 0]\n",
    "    # # calculate the probability of improvement\n",
    "    probs = acquisition_function(X, Xsamples, model)\n",
    "    return probs\n",
    "\n",
    "\n",
    "# optimize the acquisition function\n",
    "def opt_acquisition(X, y, model):\n",
    "    # random search, generate random samples\n",
    "    Xsamples = random(100)\n",
    "    Xsamples = Xsamples.reshape(len(Xsamples), 1)\n",
    "    # calculate the acquisition function for each sample\n",
    "    scores = acquisition(X, Xsamples, model)\n",
    "    # locate the index of the largest scores\n",
    "    ix = argmax(scores)\n",
    "    return Xsamples[ix, 0]\n",
    "\n",
    "\n",
    "# plot real observations vs surrogate function\n",
    "def plot(X, y, model):\n",
    "    # scatter plot of inputs and real objective function\n",
    "    pyplot.scatter(X, y)\n",
    "    # line plot of surrogate function across domain\n",
    "    Xsamples = asarray(arange(0, 1, 0.001))\n",
    "    Xsamples = Xsamples.reshape(len(Xsamples), 1)\n",
    "    ysamples, _ = surrogate(model, Xsamples)\n",
    "    pyplot.plot(Xsamples, ysamples)\n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    "\n",
    "\n",
    "def bayesianOptmization(generation):\n",
    "    # sample the domain sparsely with noise\n",
    "    X, y = initial_point(size=2)\n",
    "    # reshape into rows and cols\n",
    "    X = X.reshape(len(X), 1)\n",
    "    y = y.reshape(len(y), 1)\n",
    "    # define the model\n",
    "    model = GaussianProcessRegressor()  #you can set the kernel and the optmizer\n",
    "    # fit the model\n",
    "    model.fit(X, y)\n",
    "    # perform the optimization process\n",
    "    for i in range(generation):\n",
    "        # select the next point to sample\n",
    "        x = opt_acquisition(X, y, model)\n",
    "        # sample the point\n",
    "        actual = objective(x)\n",
    "        # summarize the finding\n",
    "        est, _ = surrogate(model, [[x]])\n",
    "        # add the data to the dataset\n",
    "        X = vstack((X, [[x]]))\n",
    "        y = vstack((y, [[actual]]))\n",
    "        # update the model\n",
    "        model.fit(X, y)\n",
    "    return X, y, model\n",
    "\n",
    "\n",
    "# objective function\n",
    "def objective(x, noise=0.1):\n",
    "    return x + normal(loc=0, scale=noise)\n",
    "\n",
    "\n",
    "#remember to return the value in the right order and type.\n",
    "def initial_point(generator_type, random, args):\n",
    "    X = generator(generator_type)(random, args)\n",
    "    Y = asarray([objective(x) for x in X])\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "# you have to add the parameters that you need.\n",
    "def acquisition_function(X, Xsamples, model):\n",
    "    # calculate the best surrogate score found so far\n",
    "    yhat, _ = surrogate(model, X)\n",
    "    best = max(yhat)\n",
    "    # calculate mean and stdev via surrogate function\n",
    "    mu, std = surrogate(model, Xsamples)\n",
    "    mu = mu[:, 0]\n",
    "    # calculate the probability of improvement\n",
    "    probs = norm.cdf((mu - best) / (std + 1E-9))\n",
    "    return probs"
   ],
   "metadata": {
    "id": "DVnijvra5FTX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Code for the Genetic algorithm from lab. 7"
   ],
   "metadata": {
    "id": "7Hg7Wyob6nr1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0qLDcl3jj0f",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "import cma\n",
    "import inspyred\n",
    "import importlib\n",
    "import functools\n",
    "import numpy as np\n",
    "from pylab import *\n",
    "from inspyred import ec\n",
    "from copy import deepcopy\n",
    "from functools import reduce\n",
    "import benchmark_functions as bf\n",
    "from matplotlib import pyplot as plt\n",
    "from inspyred.ec import EvolutionaryComputation\n",
    "from inspyred.ec import selectors, replacers, terminators\n",
    "\n",
    "GLOBAL = 'Global'\n",
    "INDIVIDUAL = 'Individual'\n",
    "CORRELATED = 'Correlated'\n",
    "STAR = 'star'\n",
    "RING = 'ring'\n",
    "\n",
    "\n",
    "class OptFun():\n",
    "    def __init__(self, wf):\n",
    "        self.f = wf\n",
    "        self.history = []\n",
    "        self.__name__ = f'OptFun({wf.__class__})'\n",
    "\n",
    "    def __call__(self, candidates, *args, **kwargs):\n",
    "        y = []\n",
    "        for x0 in candidates:\n",
    "            self.history.append(deepcopy(x0))\n",
    "            y.append(self.f(x0))\n",
    "        return y\n",
    "\n",
    "    def minima(self):\n",
    "        return self.f.minima()\n",
    "\n",
    "    def bounder(self):\n",
    "        def fcn(candidate, *args):\n",
    "            bounds = self.f.suggested_bounds()\n",
    "\n",
    "            for i, (m, M) in enumerate(zip(*bounds)):\n",
    "                if candidate[i] < m:\n",
    "                    candidate[i] = m\n",
    "                if candidate[i] > M:\n",
    "                    candidate[i] = M\n",
    "            return candidate\n",
    "\n",
    "        return fcn\n",
    "\n",
    "    def bounds(self):\n",
    "        return self._convert_bounds(self.f.suggested_bounds())\n",
    "\n",
    "    def heatmap(self, fn=None):\n",
    "        plt.clf()\n",
    "        resolution = 50\n",
    "        fig = plt.figure()\n",
    "        fig.canvas.set_window_title('Benchmark Function: ' + self.f._name)\n",
    "        fig.suptitle(self.f._name)\n",
    "        bounds_lower, bounds_upper = self.f.suggested_bounds()\n",
    "        x = np.linspace(bounds_lower[0], bounds_upper[0], resolution)\n",
    "        if self.f._n_dimensions > 1:\n",
    "            y = np.linspace(bounds_lower[1], bounds_upper[1], resolution)\n",
    "            X, Y = np.meshgrid(x, y)\n",
    "            Z = np.asarray([[self.f((X[i][j], Y[i][j])) for j in range(len(X[i]))] for i in range(len(X))])\n",
    "\n",
    "        plt.contour(x, y, Z, 15, linewidths=0.5, colors='k')  # hight lines\n",
    "        plt.contourf(x, y, Z, 15, cmap='viridis', vmin=Z.min(), vmax=Z.max())  # heat map\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        cbar = plt.colorbar()\n",
    "        cbar.set_label('z')\n",
    "        if len(self.history) > 0:  # plot points\n",
    "            xdata = [x[0] for x in self.history]\n",
    "            ydata = [x[1] for x in self.history]\n",
    "            plt.plot(xdata, ydata, \"or-\", markersize=2, linewidth=2)\n",
    "        if fn is None:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.savefig(fn, dpi=400)\n",
    "\n",
    "    def plot(self):\n",
    "        plt.clf()\n",
    "        values = [self.f(v) for v in self.history]\n",
    "        min = func.minima()[0].score\n",
    "        plt.plot(values)\n",
    "        plt.axhline(min, color=\"r\", label=\"optimum\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def _convert_bounds(self, bounds):\n",
    "        new_bounds = []\n",
    "        for i in range(len(bounds[0])):\n",
    "            new_bounds.append((bounds[0][i], bounds[1][i]))\n",
    "        return new_bounds\n",
    "\n",
    "    def current_calls(self):\n",
    "        return len(self.history)\n",
    "\n",
    "\n",
    "def choice_without_replacement(rng, n, size):\n",
    "    result = set()\n",
    "    while len(result) < size:\n",
    "        result.add(rng.randint(0, n))\n",
    "    return result\n",
    "\n",
    "\n",
    "class NumpyRandomWrapper(RandomState):\n",
    "    def __init__(self, seed=None):\n",
    "        super(NumpyRandomWrapper, self).__init__(seed)\n",
    "\n",
    "    def sample(self, population, k):\n",
    "        if isinstance(population, int):\n",
    "            population = range(population)\n",
    "\n",
    "        return asarray([population[i] for i in\n",
    "                        choice_without_replacement(self, len(population), k)])\n",
    "        #return #self.choice(population, k, replace=False)\n",
    "\n",
    "    def random(self):\n",
    "        return self.random_sample()\n",
    "\n",
    "    def gauss(self, mu, sigma):\n",
    "        return self.normal(mu, sigma)\n",
    "\n",
    "\n",
    "def initial_pop_observer(population, num_generations, num_evaluations,\n",
    "                         args):\n",
    "    if num_generations == 0:\n",
    "        args[\"initial_pop_storage\"][\"individuals\"] = asarray([guy.candidate\n",
    "                                                              for guy in population])\n",
    "        args[\"initial_pop_storage\"][\"fitnesses\"] = asarray([guy.fitness\n",
    "                                                            for guy in population])\n",
    "\n",
    "\n",
    "def generator(case):\n",
    "    if case == \"random\":\n",
    "        return random_generator\n",
    "    if case == \"LHS\":\n",
    "        return lhs_generator\n",
    "    if case == \"Halton\":\n",
    "        return Halton_generator\n",
    "    if case == \"FF\":\n",
    "        return ff_generator\n",
    "\n",
    "\n",
    "def generator_wrapper(func):\n",
    "    @functools.wraps(func)\n",
    "    def _generator(random, args):\n",
    "        return asarray(func(random, args))\n",
    "\n",
    "    return _generator\n",
    "\n",
    "\n",
    "def single_objective_evaluator(candidates, args):\n",
    "    problem = args[\"problem\"]\n",
    "    return [CombinedObjectives(fit, args) for fit in\n",
    "            problem.evaluator(candidates, args)]\n",
    "\n",
    "\n",
    "def run_ga(random, generator_type, func, num_vars=0,\n",
    "           maximize=False, **kwargs):\n",
    "    #create dictionaries to store data about initial population, and lines\n",
    "    initial_pop_storage = {}\n",
    "\n",
    "    algorithm = ec.EvolutionaryComputation(random)\n",
    "    algorithm.terminator = ec.terminators.generation_termination\n",
    "    algorithm.replacer = ec.replacers.generational_replacement\n",
    "    algorithm.variator = [ec.variators.uniform_crossover, ec.variators.gaussian_mutation]\n",
    "    algorithm.selector = ec.selectors.tournament_selection\n",
    "\n",
    "    algorithm.observer = initial_pop_observer\n",
    "\n",
    "    kwargs[\"num_selected\"] = kwargs[\"pop_size\"]\n",
    "\n",
    "    kwargs[\"bounder\"] = func.bounder()\n",
    "    kwargs[\"generator\"] = generator(generator_type)\n",
    "\n",
    "    final_pop = algorithm.evolve(evaluator=func,\n",
    "                                 maximize=False,\n",
    "                                 initial_pop_storage=initial_pop_storage,\n",
    "                                 num_vars=num_vars,\n",
    "                                 **kwargs)\n",
    "\n",
    "    #best_guy = final_pop[0].candidate\n",
    "    #best_fitness = final_pop[0].fitness\n",
    "    final_pop_fitnesses = asarray([guy.fitness for guy in final_pop])\n",
    "    final_pop_candidates = asarray([guy.candidate for guy in final_pop])\n",
    "\n",
    "    sort_indexes = sorted(range(len(final_pop_fitnesses)), key=final_pop_fitnesses.__getitem__)\n",
    "    final_pop_fitnesses = final_pop_fitnesses[sort_indexes]\n",
    "    final_pop_candidates = final_pop_candidates[sort_indexes]\n",
    "\n",
    "    best_guy = final_pop_candidates[0]\n",
    "    best_fitness = final_pop_fitnesses[0]\n",
    "\n",
    "    return best_guy, best_fitness, final_pop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Samplings methods to implement"
   ],
   "metadata": {
    "id": "3YjLWcgj7OVy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def random_generator(random, args):\n",
    "    pass\n",
    "\n",
    "\n",
    "def lhs_generator(random, args):\n",
    "    pass\n",
    "\n",
    "\n",
    "def Halton_generator(random, args):\n",
    "    pass\n",
    "\n",
    "\n",
    "def ff_generator(random, args):\n",
    "    pass"
   ],
   "metadata": {
    "id": "ic7JlvOI60fo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 1: Bayesian Optmization"
   ],
   "metadata": {
    "id": "uNxjVNUu9qBM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "X, y, model = bayesianOptmization(1)\n",
    "plot(X, y, model)\n",
    "# best result\n",
    "ix = argmax(y)\n",
    "print('Best Result: x=%.3f, y=%.3f' % (X[ix], y[ix]))"
   ],
   "metadata": {
    "id": "XNDa4uDM9ntm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 2: Genetic Algorithm"
   ],
   "metadata": {
    "id": "TE2uwsRa5sTS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from random import Random\n",
    "\n",
    "func = OptFun(bf.Ackley(2))\n",
    "\n",
    "args = {}\n",
    "args[\"num_vars\"] = 2  # Number of dimensions of the search space\n",
    "args[\"gaussian_stdev\"] = 1.0  # Standard deviation of the Gaussian mutations\n",
    "args[\"tournament_size\"] = 2\n",
    "args[\"num_elites\"] = 1  # number of elite individuals to maintain in each gen\n",
    "args[\"pop_size\"] = 20  # population size\n",
    "args[\"pop_init_range\"] = func.bounds()[0]  # Range for the initial population\n",
    "args[\"max_generations\"] = 50  # Number of generations of the GA\n",
    "args[\"crossover_rate\"] = 0.0\n",
    "args[\"mutation_rate\"] = 1.0\n",
    "\n",
    "run_ga(\n",
    "    Random(0),  # Seeded random number generator\n",
    "    func,\n",
    "    **args\n",
    ")\n",
    "\n",
    "func.heatmap()\n",
    "func.plot()"
   ],
   "metadata": {
    "id": "iX0W_9p1r7Fr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this lab, you will need to compare the algorithms studied in the previous lessons with their version enhanced with different DOE techniques.\n",
    "\n",
    "1.   How do the performances increases? Are the algorithms faster to converge, or can they find better solutions? \n",
    "2.   Is there an approach better than the others in terms of performance?\n",
    "3.   How much do the DOEs affect the search cost? \n"
   ],
   "metadata": {
    "id": "gSFPcYQirNga"
   }
  }
 ]
}
