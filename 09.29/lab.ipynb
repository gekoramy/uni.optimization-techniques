{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def folder(f: str):\n",
        "  fp = \"/content/drive/MyDrive/optimization-techniques/\" + f\n",
        "  Path(fp).mkdir(parents = True, exist_ok = True)\n",
        "  return fp"
      ],
      "metadata": {
        "id": "NfgwZ_js-B2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimization techniques Lab. 3: GD, BFGS and Newton methods\n",
        "## Introduction\n",
        "**Goal.** The goal of this laboratory is to study the application of local search algorithms on different benchmark functions.\n",
        "\n",
        "We will see the Gradient Descent, the BFGS, and the Newton methods. Moreover, we will study how their parameters change the behavior of these algorithms. \n",
        "\n",
        "**Getting started.** The following code cell contains the core functions that we will use. Hence, remember to run it every time the Colab runtime is reconnected.\n",
        "\n",
        "It contains the three local search algorithms and a wrapper class called *OptFun* for the benchmark function. \n",
        "As regards the *OptFun* class, the constructor takes as input a benchmark function (we will see later what functions are available). The relevant methods  are 4:\n",
        "1.   *Minima*: return the minimum of the function. The position can be obtained by the parameter *position* and the function value from the *score* parameter.\n",
        "2.   *Bounds*: returns where the function is defined\n",
        "3.   *Heatmap*: show a heatmap of the function highlighting the points visited by the local search (use with 2d function)\n",
        "4.   *plot*: show the trend of the points visited by the local search (use with 1d function)\n",
        "5.   *trend*: show the best points find during the optmization process. \n",
        "\n",
        "Each instance of *OptFun* stores the history of the point at which the function has been evaluated. The history is never cleaned and can be obtained through *OptFun.history*. Hence, if you reuse the class instance remember to clean the history (*OptFun.history = list()*).\n",
        "\n",
        "---\n",
        "\n",
        "The benchmark functions available comes from the *benchmark_functions* library (imported as *bf*). \n",
        "Example of the functions that can be used are the *Hypersphere*, the *Rastrign* the *DeJong5* and the Keane.\n",
        "The complete list of functions available can be found at this [link](https://gitlab.com/luca.baronti/python_benchmark_functions) or printing *dir(bf)*.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eK4fQ2q-Xcx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install benchmark_functions\n",
        "!pip install numdifftools"
      ],
      "metadata": {
        "id": "h41IdLt_Mv2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GMzkncVZcW5"
      },
      "outputs": [],
      "source": [
        "import benchmark_functions as bf\n",
        "from scipy.optimize import minimize, rosen, approx_fprime\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.misc import derivative\n",
        "import numdifftools as nd\n",
        "\n",
        "class OptFun():\n",
        "    def __init__(self, wf):\n",
        "        self.f = wf\n",
        "        self.history = []\n",
        "        \n",
        "    def __call__(self, x0):\n",
        "        #if not type(x0) == 'list':\n",
        "        #    x0 = [x0]\n",
        "        self.history.append(x0.copy())\n",
        "        return self.f(x0)\n",
        "\n",
        "    def minima(self):\n",
        "        return self.f.minimum()\n",
        "    \n",
        "    def cost(self):\n",
        "        return len(self.history)\n",
        "\n",
        "    def bounds(self):\n",
        "        return self._convert_bounds(self.f.suggested_bounds())\n",
        "    \n",
        "    def gradient(self, x):\n",
        "        return approx_fprime(x, self.f, epsilon=1.4901161193847656e-08)\n",
        "\n",
        "    def heatmap(self, fn= None):\n",
        "        plt.clf()\n",
        "        resolution = 50\n",
        "        # fig = plt.figure()\n",
        "        # fig.canvas.set_window_title('Benchmark Function: '+self.f._name)\n",
        "        # fig.suptitle(self.f._name)\n",
        "        bounds_lower, bounds_upper = self.f.suggested_bounds()\n",
        "        xdata = []\n",
        "        ydata = []\n",
        "        if len(self.history)>0:\t# plot points\n",
        "            xdata = [x[0] for x in self.history]\n",
        "            ydata = [x[1] for x in self.history]\n",
        "\n",
        "        x = np.linspace(bounds_lower[0] if len(xdata)== 0 or min(xdata)> bounds_lower[0] else min(xdata),\n",
        "                        bounds_upper[0] if len(xdata)== 0 or max(xdata)< bounds_upper[0] else max(xdata),\n",
        "                        resolution)\n",
        "        if self.f._n_dimensions>1:\n",
        "            y = np.linspace(bounds_lower[1] if len(ydata)== 0 or min(ydata)> bounds_lower[1] else min(ydata),\n",
        "                        bounds_upper[1] if len(ydata)== 0 or max(ydata)< bounds_upper[1] else max(ydata),\n",
        "                        resolution)\n",
        "            X, Y = np.meshgrid(x, y)\n",
        "            Z = np.asarray([[self.f((X[i][j],Y[i][j])) for j in range(len(X[i]))] for i in range(len(X))])\n",
        "\n",
        "        plt.contour(x,y,Z,15,linewidths=0.5,colors='k') # hight lines\n",
        "        plt.contourf(x,y,Z,15,cmap='viridis', vmin=Z.min(), vmax=Z.max()) # heat map\n",
        "        plt.xlabel('x')\n",
        "        plt.ylabel('y')\n",
        "        cbar = plt.colorbar()\n",
        "        cbar.set_label('z')\n",
        "        if len(self.history)>0:\t# plot points\n",
        "            xdata = [x[0] for x in self.history]\n",
        "            ydata = [x[1] for x in self.history]\n",
        "            plt.plot(xdata, ydata, \"or-\", markersize=2, linewidth=2)\n",
        "        if fn is None:\n",
        "            plt.show()\n",
        "        else:\n",
        "            plt.savefig(fn, dpi=400)\n",
        "\n",
        "    def trend(self, fn = None):\n",
        "        plt.clf()\n",
        "        showPoints = [x for x in self.history if self.in_bounds(x)]\n",
        "        values = [self.f(list(v)) for v in showPoints]\n",
        "        min = self.f.minimum().score\n",
        "        plt.plot(values)\n",
        "        plt.axhline(min, color=\"r\", label=\"optimum\")\n",
        "        plt.legend()\n",
        "        if fn is None:\n",
        "            plt.show()\n",
        "        else:\n",
        "            plt.savefig(fn, dpi=400)\n",
        "\n",
        "    def in_bounds(self, p):\n",
        "        bounds_lower, bounds_upper = self.f.suggested_bounds()\n",
        "        for i in range(len(p)):\n",
        "            if p[i]<bounds_lower[i] or p[i]>bounds_upper[i]:\n",
        "                return False\n",
        "        return True\n",
        "\t\t\n",
        "\n",
        "    def plot(self, fn = None):\n",
        "        plt.clf()\n",
        "        showPoints = [x for x in self.history if self.in_bounds(x)]\n",
        "        xp = [h for h in showPoints]\n",
        "        values = [self.f(v) for v in xp]\n",
        "        min = self.f.minimum().score\n",
        "        bounds = self.bounds()\n",
        "        Dtmp = [np.linspace(bounds[i][0], bounds[i][1], num=1000) for i in range(len(bounds))]\n",
        "        D = []\n",
        "        for i in range(1000):\n",
        "            D.append([Dtmp[j][i] for j in range(len(bounds))])\n",
        "        C = []\n",
        "        for i in range(1000):\n",
        "            C.append(self.f([Dtmp[j][i] for j in range(len(bounds))]))\n",
        "        plt.plot(D, C,  label = \"function\")\n",
        "        plt.plot(xp, values,  '.', color=\"r\", label=\"points\")\n",
        "        \n",
        "        plt.legend()\n",
        "        if fn is None:\n",
        "            plt.show()\n",
        "        else:\n",
        "            plt.savefig(fn, dpi=400)\n",
        "\n",
        "    def _convert_bounds(self, bounds):\n",
        "        new_bounds= []\n",
        "        for i in range(len(bounds[0])):\n",
        "            new_bounds.append((bounds[0][i], bounds[1][i]))\n",
        "        return new_bounds\n",
        "\n",
        "def gradient_descent(f, x0, learn_rate, n_iter=50, tolerance=1e-06):\n",
        "    x = x0\n",
        "    f.history.append([v for v in x])\n",
        "    for _ in range(n_iter):\n",
        "        diff = -learn_rate * f.gradient(x)\n",
        "        if np.all(np.abs(diff) <= tolerance):\n",
        "            break\n",
        "        x += diff\n",
        "        f.history.append([v for v in x])\n",
        "\n",
        "    return x\n",
        "\n",
        "def newton_method(f, x0, n_iter=50, tolerance=1e-06):\n",
        "    x = np.array(x0)\n",
        "    f.history.append(x.copy())\n",
        "    f_jacob = nd.Jacobian(f.f)\n",
        "    f_hess = nd.Hessian(f.f)\n",
        "    for _ in range(n_iter):\n",
        "        JA = f_jacob(x)\n",
        "        HA = np.linalg.inv(f_hess(x))\n",
        "        diff = -1*(JA.dot(HA)[0])\n",
        "        if np.all(np.abs(diff) <= tolerance):\n",
        "            break\n",
        "        x += diff\n",
        "        f.history.append(x.copy())\n",
        "    return x\n",
        "\n",
        "def newton_methodp(f, x0, n_iter=50, tolerance=1e-06):\n",
        "    x = np.array(x0)\n",
        "    f.history.append(x.copy())\n",
        "    f_jacob = nd.Jacobian(f.f)\n",
        "    f_hess = nd.Hessian(f.f)\n",
        "    for _ in range(n_iter):\n",
        "        JA = f_jacob(x)\n",
        "        HA = np.linalg.inv(f_hess(x))\n",
        "        print('-'*90)\n",
        "        print('JA')\n",
        "        print(JA) # derivate prime\n",
        "        print('HA')\n",
        "        print(HA) # derivate seconde\n",
        "        print('-'*90)\n",
        "        # una delle due è molto vicino allo zero, quindi va piatta e si interrompe\n",
        "        diff = -1*(JA.dot(HA)[0])\n",
        "        if np.all(np.abs(diff) <= tolerance):\n",
        "            break\n",
        "        x += diff\n",
        "        f.history.append(x.copy())\n",
        "    return x\n",
        "\n",
        "\n",
        "def bfgs(f: OptFun, x0, eps: float, maxiter: int):\n",
        "     \"\"\"\n",
        "     Optimizes a function by using the BFGS algorithm.\n",
        "\n",
        "     - f: function to optimize, an instance of OptFun\n",
        "     - x0: starting point for the search process\n",
        "     - eps: step size for the update of the jacobian\n",
        "     - maxiter: maximum number of iterations\n",
        "     \"\"\"\n",
        "     return minimize(\n",
        "         f,\n",
        "         x0,\n",
        "         method='BFGS',\n",
        "         jac=None,\n",
        "         options={\n",
        "             'gtol': 1e-05,\n",
        "             'norm': float('inf'),\n",
        "             'eps': eps,\n",
        "             \"maxiter\": maxiter,\n",
        "             'disp': False,\n",
        "             'return_all': True,\n",
        "             'finite_diff_rel_step': None\n",
        "         }\n",
        "     )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bfs = [\n",
        "    (\"de jong 3\", bf.DeJong3()),\n",
        "    (\"de jong 5\", bf.DeJong5()),\n",
        "    (\"picheny goldstein and price\", bf.PichenyGoldsteinAndPrice()),\n",
        "    (\"ackley\", bf.Ackley()),\n",
        "    (\"griewank\", bf.Griewank(zoom = 2)),\n",
        "    (\"michalewicz\", bf.Michalewicz()),\n",
        "    (\"keane\", bf.Keane())\n",
        "  ]\n",
        "\n",
        "lab = \"09 29/\""
      ],
      "metadata": {
        "id": "mPT8wJvJ-WpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ex. 1: Gradient Descent\n",
        "---\n",
        "In this first exercise we will focus on the Gradient Descent algorithm. \n",
        "\n",
        "\n",
        "*   How the Learning Rate influences the optmization?\n",
        "*   How the tollerance influeces the search? \n",
        "*   The effects of these parameters is the same across different functions?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T4i9MRSlId1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for learn_rate in [0.05, 0.2, 0.5]:\n",
        "  for tolerance in [1e-4]:\n",
        "\n",
        "    func = OptFun(bf.Hyperellipsoid())\n",
        "    gradient_descent(\n",
        "        f = func,\n",
        "        x0 = np.array([-60., -60.]),\n",
        "        learn_rate = learn_rate,\n",
        "        n_iter = 80,\n",
        "        tolerance = tolerance\n",
        "    )\n",
        "\n",
        "    print(f'learn_rate:{learn_rate} tolerance:{tolerance}')\n",
        "    func.heatmap()\n",
        "    func.trend()"
      ],
      "metadata": {
        "id": "L15x3OXJcXL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VIm9IDY9vbHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for learn_rate in [0.2]:\n",
        "  for tolerance in [1e2, 1e1, 1, 1e-1, 1e-2]:\n",
        "\n",
        "    func = OptFun(bf.Hyperellipsoid())\n",
        "    gradient_descent(\n",
        "        f = func,\n",
        "        x0 = np.array([-60., -60.]),\n",
        "        learn_rate = learn_rate,\n",
        "        n_iter = 80,\n",
        "        tolerance = tolerance\n",
        "    )\n",
        "\n",
        "    print(f'learn_rate:{learn_rate} tolerance:{tolerance}')\n",
        "    func.heatmap()\n",
        "    func.trend()"
      ],
      "metadata": {
        "id": "R2J5fPs5vbiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for learn_rate in [0.5, 5]:\n",
        "  for tolerance in [1e-2, 1e-10]:\n",
        "\n",
        "    func = OptFun(bf.Ackley())\n",
        "    gradient_descent(\n",
        "        f = func,\n",
        "        x0 = np.array([-60., -60.]),\n",
        "        learn_rate = learn_rate,\n",
        "        n_iter = 80,\n",
        "        tolerance = tolerance\n",
        "    )\n",
        "\n",
        "    print(f'learn_rate:{learn_rate} tolerance:{tolerance}')\n",
        "    func.heatmap()\n",
        "    func.trend()"
      ],
      "metadata": {
        "id": "sAjSg-dtxzQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ex. 3: Newton Method\n",
        "---\n",
        "In this exercise we will see the Newton method.\n",
        "Similar to the previous exercise, answer the following questions:\n",
        "\n",
        "*   How the tollerance influeces the search?\n",
        "*   Is it faster to converge with respect to GD?\n",
        "*   The result are similar across different functions?\n"
      ],
      "metadata": {
        "id": "1PiMsaY6AYWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for tolerance in [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]:\n",
        "  func = OptFun(bf.Rosenbrock())\n",
        "  newton_method(\n",
        "      f = func, \n",
        "      x0 = np.array([2., -2.]), \n",
        "      n_iter = 100, \n",
        "      tolerance = tolerance\n",
        "    )\n",
        "    \n",
        "  print(f'tolerance:{tolerance}')\n",
        "  func.heatmap()\n",
        "  func.trend()"
      ],
      "metadata": {
        "id": "_LQbq-6kpqe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tolerance in [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]:\n",
        "  func = OptFun(bf.Hyperellipsoid())\n",
        "  newton_method(\n",
        "      f = func, \n",
        "      x0 = np.array([-60., -60.]), \n",
        "      n_iter = 100, \n",
        "      tolerance = tolerance\n",
        "    )\n",
        "    \n",
        "  print(f'tolerance:{tolerance}')\n",
        "  func.heatmap()\n",
        "  func.trend()"
      ],
      "metadata": {
        "id": "2LQeSV4q2gld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tolerance in [1e-6]:\n",
        "  func = OptFun(bf.Keane())\n",
        "  newton_methodp(\n",
        "      f = func, \n",
        "      x0 = np.array([5., 7.]), \n",
        "      n_iter = 1000, \n",
        "      tolerance = tolerance\n",
        "    )\n",
        "    \n",
        "  print(f'tolerance:{tolerance}')\n",
        "  func.heatmap()\n",
        "  func.trend()\n",
        "\n",
        "  # una tra JA e HA si trova vicino allo zero, quindi conclude lo o. a."
      ],
      "metadata": {
        "id": "to_cKyUJ3QBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ex. 2: BFGS  Optimization\n",
        "---\n",
        "In this exercise we will focus on the BFGS optimization algorithm.\n",
        "Similar to the previous exercise, answer the following questions:\n",
        "\n",
        "* Varying theses parameters what happens?\n",
        "* How they influence the evolution process?\n",
        "* What are the difference with L-BFGS? Hint: you have to change the bfgs function, calling the right minimize method. See [link text](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-lbfgsb.html#optimize-minimize-lbfgsb) to see the parameters avaiable.\n"
      ],
      "metadata": {
        "id": "tVnX_NOe-Hh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(bf))\n",
        "func = OptFun(bf.Hyperellipsoid(2))\n",
        "\n",
        "results = bfgs(func, [-60.,-60.], 0.001, 50)\n",
        "func.trend()\n",
        "func.heatmap()"
      ],
      "metadata": {
        "id": "aWvpCl5OePDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instructions and questions\n",
        "---\n",
        "Concisely note down your observations from the previous exercises (follow the bullet points) and\n",
        "think about the following questions.\n",
        "* What is the difference in search cost (the number of function/derivative evaluations) between these methods? \n",
        "* Comparing these methods to the ones of the first laboratory, are they faster? Or find the optimal more efficiently?  \n",
        " "
      ],
      "metadata": {
        "id": "oi-Y6K0DBKzh"
      }
    }
  ]
}