{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def folder(f: str):\n",
        "  fp = \"/content/drive/MyDrive/optimization-techniques/\" + f\n",
        "  Path(fp).mkdir(parents = True, exist_ok = True)\n",
        "  return fp"
      ],
      "metadata": {
        "id": "RiRRz1ac1qVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimization techniques Lab. 1: Local search\n",
        "## Introduction\n",
        "**Goal.** The goal of this laboratory is to study the application of local search algorithms on different benchmark functions.\n",
        "\n",
        "We will see the Powell, the BFGS, and the Nelder Mead methods. Moreover, we will study how their parameters change the behavior of these algorithms. \n",
        "\n",
        "**Getting started.** The following code cell contains the core functions that we will use. Hence, remember to run it every time the Colab runtime is reconnected.\n",
        "\n",
        "It contains the three local search algorithms and a wrapper class called *OptFun* for the benchmark function. \n",
        "As regards the *OptFun* class, the constructor takes as input a benchmark function (we will see later what functions are available). The relevant methods  are 4:\n",
        "1.   *Minima*: return the minimum of the function. The position can be obtained by the parameter *position* and the function value from the *score* parameter.\n",
        "2.   *Bounds*: returns where the function is defined\n",
        "3.   *Heatmap*: show a heatmap of the function highlighting the points visited by the local search (use with 2d function)\n",
        "4.   *plot*: show the trend of the points visited by the local search (use with 1d function)\n",
        "5.   *trend*: show the best points find during the optmization process. \n",
        "\n",
        "Each instance of *OptFun* stores the history of the point at which the function has been evaluated. The history is never cleaned and can be obtained through *OptFun.history*. Hence, if you reuse the class instance remember to clean the history (*OptFun.history = list()*).\n",
        "\n",
        "---\n",
        "\n",
        "The benchmark functions available comes from the *benchmark_functions* library (imported as *bf*). \n",
        "Example of the functions that can be used are the *Hypersphere*, the *Rastrign* the *DeJong5* and the Keane.\n",
        "The complete list of functions available can be found at this [link](https://gitlab.com/luca.baronti/python_benchmark_functions) or you can print it with *dir(bf)*.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eK4fQ2q-Xcx1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GMzkncVZcW5"
      },
      "outputs": [],
      "source": [
        "!pip3 install benchmark_functions\n",
        "import benchmark_functions as bf\n",
        "from scipy.optimize import minimize, rosen\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "class OptFun():\n",
        "    def __init__(self, wf):\n",
        "        self.f = wf\n",
        "        self.history = []\n",
        "        \n",
        "    def __call__(self, x0):\n",
        "        self.history.append(x0.copy())\n",
        "        return self.f(x0)\n",
        "\n",
        "    def minima(self):\n",
        "        return self.f.minimum()\n",
        "        \n",
        "    def bounds(self):\n",
        "        return self._convert_bounds(self.f.suggested_bounds())\n",
        "\n",
        "    def in_bounds(self, p):\n",
        "        bounds_lower, bounds_upper = self.f.suggested_bounds()\n",
        "        for i in range(len(p)):\n",
        "            if p[i]<bounds_lower[i] or p[i]>bounds_upper[i]:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def heatmap(self, fn = None):\n",
        "        plt.clf()\n",
        "        resolution = 50\n",
        "        fig = plt.figure()\n",
        "        fig.canvas.set_window_title('Benchmark Function: '+self.f._name)\n",
        "        fig.suptitle(self.f._name)\n",
        "        bounds_lower, bounds_upper = self.f.suggested_bounds()\n",
        "        x = np.linspace(bounds_lower[0], bounds_upper[0], resolution)\n",
        "        if self.f._n_dimensions>1:\n",
        "            y = np.linspace(bounds_lower[1], bounds_upper[1], resolution)\n",
        "            X, Y = np.meshgrid(x, y)\n",
        "            Z = np.asarray([[self.f((X[i][j],Y[i][j])) for j in range(len(X[i]))] for i in range(len(X))])\n",
        "\n",
        "        plt.contour(x,y,Z,15,linewidths=0.5,colors='k') # hight lines\n",
        "        plt.contourf(x,y,Z,15,cmap='viridis', vmin=Z.min(), vmax=Z.max()) # heat map\n",
        "        plt.xlabel('x')\n",
        "        plt.ylabel('y')\n",
        "        cbar = plt.colorbar()\n",
        "        cbar.set_label('z')\n",
        "        if len(self.history)>0:\t# plot points\n",
        "            xdata = [x[0] for x in self.history]\n",
        "            ydata = [x[1] for x in self.history]\n",
        "            plt.plot(xdata, ydata, \"or-\", markersize=2, linewidth=2)\n",
        "        if fn is None:\n",
        "            plt.show()\n",
        "        else:\n",
        "            plt.savefig(fn, dpi=400)\n",
        "\n",
        "    def plot(self, fn =None):\n",
        "        plt.clf()\n",
        "        showPoints = [x for x in self.history if self.in_bounds(x)]\n",
        "        xp = [h for h in showPoints]\n",
        "        values = [self.f(v) for v in xp]\n",
        "        min = self.f.minimum().score\n",
        "        bounds = self.bounds()\n",
        "        Dtmp = [np.linspace(bounds[i][0], bounds[i][1], num=1000) for i in range(len(bounds))]\n",
        "        D = []\n",
        "        for i in range(1000):\n",
        "            D.append([Dtmp[j][i] for j in range(len(bounds))])\n",
        "        C = []\n",
        "        for i in range(1000):\n",
        "            C.append(self.f([Dtmp[j][i] for j in range(len(bounds))]))\n",
        "        plt.plot(D, C,  label = \"function\")\n",
        "        plt.plot(xp, values,  '.', color=\"r\", label=\"points\")\n",
        "        \n",
        "        plt.legend()\n",
        "        if fn is None:\n",
        "            plt.show()\n",
        "        else:\n",
        "            plt.savefig(fn, dpi=400)\n",
        "            \n",
        "    def trend(self, fn = None):\n",
        "        plt.clf()\n",
        "        showPoints = [x for x in self.history if self.in_bounds(x)]\n",
        "        values = [self.f(list(v)) for v in showPoints]\n",
        "        min = self.f.minimum().score\n",
        "        plt.plot(values)\n",
        "        plt.axhline(min, color=\"r\", label=\"optimum\")\n",
        "        plt.legend()\n",
        "        if fn is None:\n",
        "            plt.show()\n",
        "        else:\n",
        "            plt.savefig(fn, dpi=400)\n",
        "\n",
        "    def _convert_bounds(self, bounds):\n",
        "        new_bounds= []\n",
        "        for i in range(len(bounds[0])):\n",
        "            new_bounds.append((bounds[0][i], bounds[1][i]))\n",
        "        return new_bounds\n",
        "\n",
        "def powell(f: OptFun, x0, maxiter: int):\n",
        "    \"\"\"\n",
        "    Optimizes a function by using the Powell algorithm.\n",
        "\n",
        "    - f: function to optimize, an instance of OptFun\n",
        "    - x0: starting point for the search process\n",
        "    - maxiter: maximum number of iterations\n",
        "    \"\"\"\n",
        "    bounds = f.bounds()\n",
        "    results = minimize(fun=f, x0=list(x0), method=\"powell\", bounds=bounds, \n",
        "                       options={\"ftol\":1e-4,\n",
        "                                \"maxfev\": None,\n",
        "                                \"maxiter\": maxiter,\n",
        "                                \"return_all\":True})\n",
        "    return results\n",
        "\n",
        "def nelder_mead(f: OptFun, x0, maxiter: int):\n",
        "    \"\"\"\n",
        "    Optimizes a function by using the Nelder-Mead algorithm.\n",
        "\n",
        "    - f: function to optimize, an instance of OptFun\n",
        "    - x0: starting point for the search process\n",
        "    - maxiter: maximum number of iterations\n",
        "    \"\"\"\n",
        "    bounds = f.bounds()\n",
        "    return minimize(\n",
        "        f,\n",
        "        x0,\n",
        "        method='Nelder-Mead',\n",
        "        tol=None,\n",
        "        bounds=bounds,\n",
        "        options={\n",
        "            \"maxfev\": None,\n",
        "            \"maxiter\": maxiter,\n",
        "            'disp': False,\n",
        "            'return_all': True,\n",
        "            'initial_simplex': None,\n",
        "            'xatol': 0.000,\n",
        "            'fatol': 0.000,\n",
        "            'adaptive': False\n",
        "        }\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ex 1: Grid Search\n",
        "---\n",
        "In this first exercise we will use grid search as a search algorithm\n",
        "## Questions\n",
        "- How does the step size influence the quality of the best point obtained?\n",
        "- How does the step size influence the search cost?"
      ],
      "metadata": {
        "id": "eZAAE0sm3iXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "func = OptFun(bf.Hypersphere(2))\n",
        "bounds = func.bounds()\n",
        "step_size = 1\n",
        "best = float('inf')\n",
        "for x in np.arange(bounds[0][0], bounds[0][1], step_size):\n",
        "    for y in np.arange(bounds[1][0], bounds[1][1], step_size):\n",
        "        current = func([x, y])\n",
        "        if current < best:\n",
        "            best = current\n",
        "func.heatmap()\n",
        "func.plot()"
      ],
      "metadata": {
        "id": "IZ3J9nRw3h_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ex 2: Random Search\n",
        "---\n",
        "In this exercise we will use Random Search to search for the optimum.\n",
        "## Questions\n",
        "- How does the number of samples drawn affect the search?\n",
        "- How does this method compare to Grid Search? What are the advantages and disadvantages?"
      ],
      "metadata": {
        "id": "o4YUAK5445-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "func = OptFun(bf.Hypersphere(2))\n",
        "bounds = func.bounds()\n",
        "n_samples_drawn = 30\n",
        "best = float('inf')\n",
        "for i in range(n_samples_drawn):\n",
        "    x = np.random.uniform(bounds[0][0], bounds[0][1])\n",
        "    y = np.random.uniform(bounds[1][0], bounds[1][1])\n",
        "    current = func([x, y])\n",
        "    if current < best:\n",
        "        best = current\n",
        "func.heatmap()\n",
        "func.plot()"
      ],
      "metadata": {
        "id": "rS-LssKy4811"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ex. 3: Powell Optimization\n",
        "---\n",
        "In this exercise we will focus on the Powel optimization algorithm. \n",
        "\n",
        "## Questions\n",
        "*   What happens when varying the parameters of the algorithm?\n",
        "*   How they influence the optimization process? \n",
        "*   The effects of these parameters is the same across different functions?\n",
        "*   How does this algorithm compare to the previous?\n",
        "\n",
        "parameters:\n",
        "*   fun: function to optimize\n",
        "*   $x_0$: initial point\n",
        "*   maxiter: maximum number of iteration\n",
        "\n"
      ],
      "metadata": {
        "id": "T4i9MRSlId1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(bf))\n",
        "func = OptFun(bf.Hypersphere(2))\n",
        "a = powell(func,  (-0.,-0.), 1)\n",
        "func.heatmap()\n",
        "func.plot()"
      ],
      "metadata": {
        "id": "L15x3OXJcXL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ex. 4: Nelder Mead Optimization\n",
        "---\n",
        "In this exercise we will focus on the Nelder Mead optimization algorithm.\n",
        "Similar to the previous exercise, answer the following questions:\n",
        "\n",
        "## Questions\n",
        "*   What happens when varying the parameters of the algorithm?\n",
        "*   How they influence the optimization process? \n",
        "*   The effects of these parameters is the same across different functions?\n",
        "*   How does this algorithm compare to the previous?\n",
        "\n",
        "parameters:\n",
        "\n",
        "* fun: function to optimize\n",
        "* 𝑥0 : initial point\n",
        "* maxiter: maximum number of iteration"
      ],
      "metadata": {
        "id": "1PiMsaY6AYWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "func = OptFun(bf.Hypersphere(2))\n",
        "results = nelder_mead(func, [0,0], 1)\n",
        "func.heatmap()\n",
        "func.plot()"
      ],
      "metadata": {
        "id": "_LQbq-6kpqe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instructions and questions\n",
        "---\n",
        "Concisely note down your observations from the previous exercises (follow the bullet points) and\n",
        "think about the following questions.\n",
        "\n",
        "* How the benchmark functions influence the optimization algorithms? There is an algorithm which is always better than the other?\n",
        "* The choiche of the parameters is influenced by the function to optimize? And how the algorithm are influenced by the parameters? \n"
      ],
      "metadata": {
        "id": "oi-Y6K0DBKzh"
      }
    }
  ]
}